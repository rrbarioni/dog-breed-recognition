{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2dec0f0a5f7b4b31be43a31a80ad5e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68540bd20ea74ee4840afa8289434c8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52454491a6bd4da59bd52f58c5288dc6",
              "IPY_MODEL_578d2847af19497b9c8548b0bdaec955"
            ]
          }
        },
        "68540bd20ea74ee4840afa8289434c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52454491a6bd4da59bd52f58c5288dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4643c9ed38cd4e91b317bb886b541c2a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95ecdbed0d3466190846865a90842b8"
          }
        },
        "578d2847af19497b9c8548b0bdaec955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a418aa492fb4194aeadf0ab7bc8dfa2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 81.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df7af08bcd4c4952ab314bfdac1780d5"
          }
        },
        "4643c9ed38cd4e91b317bb886b541c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95ecdbed0d3466190846865a90842b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a418aa492fb4194aeadf0ab7bc8dfa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df7af08bcd4c4952ab314bfdac1780d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUc9OcDGevGo"
      },
      "source": [
        "# Dog Embeddings Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYII1KExe7-g"
      },
      "source": [
        "## Accessing the Dog Breed Recognition dataset\n",
        "\n",
        "I have created a directory called \"dog-breed-recognition\". There, I have put the directory called \"dogs\" as refering to the dataset itself. For training, it is only used the samples contained at \"train\" directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ987NSktMPV",
        "outputId": "5eee7f06-81f6-4a75-bea5-cccda0186463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "root = '/content/drive/My Drive/Colab Notebooks/dog-breed-recognition'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vb0U4WCe-ka"
      },
      "source": [
        "## Importing basic Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFX4dbG1tWQ3"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tqdm\n",
        "import random\n",
        "import copy\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCr72MgvfDI9"
      },
      "source": [
        "## Importing PyTorch library\n",
        "\n",
        "For GPU usage, go to \"Edit > Notebook Settings\" and make sure the hardware accelerator is set to GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcK7avytojN"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# Creating a PyTorch device, so that inputs, outputs and models are apllied to\n",
        "#   the available GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZuWYtVjfoBr"
      },
      "source": [
        "## Splitting dataset into training, validation and test\n",
        "\n",
        "Considering a split ratio between these three categories, the instances of each class (dog breed) are randomly distributed.\n",
        "\n",
        "`dataset_labels[<PHASE>]` is a list of `(class_index, instance_index)` occurences, where:\n",
        "- `class_index` refers to the index of its dog breed;\n",
        "- `instance_index` refers to the index of the instance at the current dog breed list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rYoxKB0tzBc"
      },
      "source": [
        "def get_dataset_split_labels(dataset_path, split_ratio):\n",
        "  '''\n",
        "  Calculates the split ratio of the training, validation and test sets.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_path : str\n",
        "    root of the dataset\n",
        "  split_ratio : list<float>\n",
        "    ratios for the dataset splitting\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dataset_labels : dict\n",
        "    list of instance labels for each set\n",
        "  '''\n",
        "\n",
        "  # Attribute `split_ratio` to each set individually\n",
        "  train_ratio, val_ratio, test_ratio = split_ratio\n",
        "\n",
        "  # `dataset_labels` encodes the list of instance labels for each set\n",
        "  dataset_labels = { 'train': [], 'val': [], 'test': [] }\n",
        "\n",
        "  # `dataset_path` divides the dataset in a list of directories, where each\n",
        "  #   directory represent a class (dog breed). When listing the presented\n",
        "  #   directories in `dataset_path`, `classes` will contain the list of dog\n",
        "  #   breeds presented in the dataset\n",
        "  classes = sorted(os.listdir(dataset_path))\n",
        "  \n",
        "  # Iterate through each existing class (`curr_class`) and its index (`i_class`)\n",
        "  #   (the dataset splitting is done for each class individually)\n",
        "  for i_class, curr_class in enumerate(classes):\n",
        "\n",
        "    # `class_path` appends the root of the dataset (`dataset_path`) to the\n",
        "    #   current class' directory name    \n",
        "    class_path = os.path.join(dataset_path, curr_class)\n",
        "    \n",
        "    # `instances` list all images' filename of the current class directory\n",
        "    instances = sorted(os.listdir(class_path))\n",
        "    \n",
        "    # `n_instances` computes the number of instances of the presented class\n",
        "    n_instances = len(instances)\n",
        "\n",
        "    # `labels` encodes a list of pairs of class index and instance index, which\n",
        "    #   will be used when loading the dataset later\n",
        "    labels = [(i_class, label) for label in list(range(n_instances))]\n",
        "    \n",
        "    # randomize the labels occurences for dataset splitting afterwards\n",
        "    random.shuffle(labels)\n",
        "\n",
        "    # Calculate the number of instances of each split for the current class\n",
        "    train_l = int(n_instances * train_ratio)\n",
        "    val_l = int(n_instances * val_ratio)\n",
        "    test_l = int(n_instances * test_ratio)\n",
        "\n",
        "    # Access the current labels list (`labels`) according to the number of\n",
        "    #   instances of each split and apply to the list of labels of each split\n",
        "    #   (these three sets are disjoints)\n",
        "    curr_train_labels = labels[:train_l]\n",
        "    curr_val_labels = labels[train_l:train_l + val_l]\n",
        "    curr_test_labels = labels[train_l + val_l:train_l + val_l + test_l]\n",
        "    \n",
        "    # Apply the current labels lists to the final list containing all classses\n",
        "    dataset_labels['train'] += curr_train_labels\n",
        "    dataset_labels['val'] += curr_val_labels\n",
        "    dataset_labels['test'] += curr_test_labels\n",
        "\n",
        "  return dataset_labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1t60Q8gR52"
      },
      "source": [
        "## Creating the dataset loader\n",
        "\n",
        "- For reading an entry from the dataset from an index:\n",
        "- `class_index` and `instance_index` are obtained from the previously generated labels;\n",
        "- The image path is obtained (`img_path`);\n",
        "- The image is read and converted to RGB channels (`img`), just in case the original image has a transparency channel (which will be not used) or the original image is in grayscale;\n",
        "- The network input (`x`) is generated by preprocessing the image. This preprocess depends of the current phase (training, validation or test), since the training phase deals with data augmentation.\n",
        "\n",
        "This dataset loader aims to provide samples considering the Triplet Margin Loss metric, which consists of:\n",
        "- An anchor image, which is the image refered to the accessed index;\n",
        "- A positive image, which refers to an image with the same class (dog breed) as the anchor image;\n",
        "- A negative image, which refers to an image with a different class from the anchor image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5n_VQ8bxKm7"
      },
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  A class to read the dataset instances.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  labels : list<tuple<int,int>>\n",
        "    list of dataset instances that can be accessed\n",
        "  transform : torch.transforms\n",
        "    input preprocessing pipeline\n",
        "  n_classes : int\n",
        "    number of existing classes (dog breeds) on the dataset\n",
        "  instances_path : list<list<str>>\n",
        "    Path to the instances of the dataset presented in `labels`\n",
        "\n",
        "  Data descriptors\n",
        "  ----------------\n",
        "  __getitem__\n",
        "    Gets the model's triplet inputs from a dataset instance's index.\n",
        "\n",
        "  __len__\n",
        "    Gets the number of samples presented in the dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataset_path, labels, transform):\n",
        "    '''\n",
        "    Constructs all the attributes for the dataset object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_path : str\n",
        "      root of the dataset\n",
        "    labels : list<tuple<int,int>>\n",
        "      list of dataset instances that can be accessed\n",
        "    transform : torch.transforms\n",
        "      input preprocessing pipeline\n",
        "    '''\n",
        "\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "    # `dataset_path` divides the dataset in a list of directories, where each\n",
        "    #   directory represent a class (dog breed). When listing the presented\n",
        "    #   directories in `dataset_path`, `self.classes` will contain the list of\n",
        "    #   dog breeds presented in the dataset\n",
        "    classes = sorted(os.listdir(dataset_path))\n",
        "    self.n_classes = len(classes)\n",
        "\n",
        "    # `classes_path` appends the root of the dataset (`dataset_path`) to the\n",
        "    #   directory name of all classes (`classes`)\n",
        "    classes_path = [os.path.join(dataset_path, c) for c in classes]\n",
        "    self.instances_path = [[os.path.join(class_path, instance)\n",
        "        for instance in sorted(os.listdir(class_path))]\n",
        "      for class_path in classes_path]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    '''\n",
        "    Gets the model's triplet inputs from a dataset instance's index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    index : int\n",
        "      index of the instance to be accessed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    anc_x : torch.Tensor\n",
        "      tensor refering to the preprocessed anchor sample\n",
        "    pos_x : torch.Tensor\n",
        "      tensor refering to the preprocessed positive sample\n",
        "    neg_x : torch.Tensor\n",
        "      tensor refering to the preprocessed negative sample\n",
        "    '''\n",
        "\n",
        "    # Access the indexes of the class (`anc_class_index`) and instance\n",
        "    #   (`anc_instance_index`) of the anchor image, present in the labels\n",
        "    anc_class_index, anc_instance_index = self.labels[index]\n",
        "    \n",
        "    # Access the indexes of the class (`pos_class_index`) and instance\n",
        "    #   (`pos_instance_index`) of the positive image\n",
        "    # As the positive image is from the same dog breed as the anchor image,\n",
        "    #   they have the same class index\n",
        "    pos_class_index = anc_class_index\n",
        "\n",
        "    # As the positive image and the anchor image are not the same, a different\n",
        "    #   image from the same dog breed is randomly selected\n",
        "    pos_instance_index = random.choice([instance_index\n",
        "        for instance_index in range(len(self.instances_path[pos_class_index]))\n",
        "        if instance_index != anc_instance_index])\n",
        "    \n",
        "    # Access the indexes of the class (`neg_class_index`) and instance\n",
        "    #   (`neg_instance_index`) of the negative image\n",
        "    # As the negative image is from a different dog breed as the anchor image,\n",
        "    #   a image from a randomly different dog breed is randomly selected\n",
        "    neg_class_index = random.choice([class_index\n",
        "        for class_index in range(self.n_classes)\n",
        "        if class_index != anc_class_index])\n",
        "    neg_instance_index = random.choice(\n",
        "        range(len(self.instances_path[neg_class_index])))\n",
        "    \n",
        "    # `[type]_img_path` refers to the filepath of the image refering to\n",
        "    #   (`[type]_class_index`, `[type]_instance_index`)\n",
        "    anc_img_path = self.instances_path[anc_class_index][anc_instance_index]\n",
        "    pos_img_path = self.instances_path[pos_class_index][pos_instance_index]\n",
        "    neg_img_path = self.instances_path[neg_class_index][neg_instance_index]\n",
        "    \n",
        "    # Read image (`[type]_img`) and convert to red-green-blue channels (RGB),\n",
        "    #   ensuring the inputs will have 3 channels\n",
        "    anc_img = Image.open(anc_img_path).convert('RGB')\n",
        "    pos_img = Image.open(pos_img_path).convert('RGB')\n",
        "    neg_img = Image.open(neg_img_path).convert('RGB')\n",
        "\n",
        "    # `[type]_x` refers to the image when the preprocessing pipeline\n",
        "    #   (`self.transform`) is applied to the image (`[type]_img`)\n",
        "    anc_x = self.transform(anc_img)\n",
        "    pos_x = self.transform(pos_img)\n",
        "    neg_x = self.transform(neg_img)\n",
        "\n",
        "    return anc_x, pos_x, neg_x\n",
        "\n",
        "  def __len__(self):\n",
        "    '''\n",
        "    Gets the number of samples presented in the dataset.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    l : int\n",
        "      the length of the dataset\n",
        "    '''\n",
        "    \n",
        "    l = len(self.labels)\n",
        "    \n",
        "    return l"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nj82WH8-MXT"
      },
      "source": [
        "## Creating the CNN model architecture\n",
        "\n",
        "The model shall have a fixed input size with 3 channels (corresponding to the red, green and blue channels). Also, the model shall output a vector with a size of `n_embeddings`, representing the embeddings from the input image.\n",
        "\n",
        "A ResNet50-based model was used; its last layer (which corresponds to a fully connecter layer) is replaced by another FCL whose output size correspond to `n_embeddings`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hYQtkAy5uYR"
      },
      "source": [
        "def embedder_model(n_embeddings):\n",
        "  '''\n",
        "  Generates a new CNN ResNet50-based model.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_embeddings : int\n",
        "    number of embeddings to be outputted\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x : torch.nn\n",
        "    the model\n",
        "  '''\n",
        "\n",
        "  # First, `x` is a new ResNet50 CNN model, containing pre-trained weights from\n",
        "  #   ImageNet\n",
        "  x = torchvision.models.resnet50(pretrained=True)\n",
        "  \n",
        "  # Change the final fully connected layer so that the output size matches the\n",
        "  #   desired `n_embeddings` size. Also, apply sigmoid function, as suggested by\n",
        "  #   the Lossless Triplet Loss approach\n",
        "  x.fc = torch.nn.Sequential(\n",
        "      torch.nn.Linear(2048, n_embeddings),\n",
        "      torch.nn.Sigmoid())\n",
        "\n",
        "  return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXviWI85XSTE"
      },
      "source": [
        "## Lossless Triplet Loss\n",
        "\n",
        "Based on https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfOhXBvJXQ5d"
      },
      "source": [
        "class LosslessTripletLoss(torch.nn.Module):\n",
        "  def __init__(self, n_embeddings):\n",
        "    super(LosslessTripletLoss, self).__init__()\n",
        "    self.epsilon = 1e-8\n",
        "    self.n_embeddings = n_embeddings\n",
        "    self.beta = n_embeddings\n",
        "      \n",
        "  def get_distance(self, x1, x2):\n",
        "    dist = (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "    return dist\n",
        "\n",
        "  def forward(self, anc, pos, neg):\n",
        "    anc_pos_dist = self.get_distance(anc, pos)\n",
        "    anc_neg_dist = self.get_distance(anc, neg)\n",
        "\n",
        "    anc_pos_dist = -torch.log(-torch.div(anc_pos_dist, self.beta) + 1 + self.epsilon)\n",
        "    anc_neg_dist = -torch.log(-torch.div((self.n_embeddings - anc_neg_dist), self.beta) + 1 + self.epsilon)\n",
        "\n",
        "    loss = anc_neg_dist + anc_pos_dist\n",
        "\n",
        "    loss = loss.mean()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl787cgM-l1s"
      },
      "source": [
        "## Model training and validation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4tFKwO65rC"
      },
      "source": [
        "def train(model, criterion, optimizer, n_epochs):\n",
        "  '''\n",
        "  Trains the model.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn\n",
        "    the model to be trained\n",
        "  criterion : torch.nn\n",
        "    the model's loss metric\n",
        "  optimizer : torch.optim\n",
        "    pptimization algorithm\n",
        "  n_epochs : int\n",
        "    number of iterations of the training\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  model : torch.nn\n",
        "    the trained model\n",
        "  best_acc : float\n",
        "    the accuracy of the trained model\n",
        "  '''\n",
        "  \n",
        "  # Keep track of the best achieved accuracy, loss and the corresponding model\n",
        "  #   weights\n",
        "  best_weights = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = float('inf')\n",
        "\n",
        "  # The model iterates a number of times (`n_epochs`)\n",
        "  for epoch in range(n_epochs):\n",
        "    \n",
        "    # Each epoch includes the model weights tuning phase (indicated by the\n",
        "    #   `train` flag) and the model validation (indicated by the `val` flag)\n",
        "    for phase in ['train', 'val']:\n",
        "      \n",
        "      # Change the model mode (training or validation)\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      # `epoch_loss` computes the loss sum (according to the used `criterion`)\n",
        "      #   of the model iteratively as batches are read\n",
        "      epoch_loss = 0.0\n",
        "      \n",
        "      # `epoch_acc` computes the accuracy of the model when comparing the\n",
        "      #   difference between anchor-positive embeddings distance and anchor\n",
        "      #   negative embeddings distance\n",
        "      epoch_acc = 0.0\n",
        "      \n",
        "      # `n_seen_samples` computes the number of triplet input samples that were\n",
        "      #   already read in the current epoch-phase\n",
        "      n_seen_samples = 0\n",
        "\n",
        "      # Using tqdm to iteratively keep track on the number of iterated batches\n",
        "      #   on the console\n",
        "      dataloader = tqdm.tqdm(dataloaders[phase], total=len(dataloaders[phase]),\n",
        "          position=0, leave=True)\n",
        "      \n",
        "      # The dataloader refering to the current phase is iterated, in order to\n",
        "      #   access all triplets of anchor-positive-negative inputs, denoted by\n",
        "      #   `(anc_x, pos_x, neg_x)`\n",
        "      for anc_x, pos_x, neg_x in dataloader:\n",
        "\n",
        "        # `curr_batch_size` computes the number of samples in the current batch\n",
        "        #   (this may vary when the current batch is the last one)\n",
        "        curr_batch_size = anc_x.shape[0]\n",
        "        \n",
        "        # Increment the number of seen samples on the current epoch-phase\n",
        "        n_seen_samples += curr_batch_size\n",
        "\n",
        "        # Reset current gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass the triplet inputs tensors to the used device (GPU or CPU)\n",
        "        anc_x = anc_x.to(device)\n",
        "        pos_x = pos_x.to(device)\n",
        "        neg_x = neg_x.to(device)\n",
        "\n",
        "        # Use the model to calculate the embeddings of the triplet inputs\n",
        "        #   (`anc_y`, `pos_y` and `neg_y`)\n",
        "        anc_y = model(anc_x)\n",
        "        pos_y = model(pos_x)\n",
        "        neg_y = model(neg_x)\n",
        "\n",
        "        # Calculate the loss of the current batch (according to the `criterion`\n",
        "        #   used) with respect to the triplet outputs `(anc_y, pos_y, neg_y)`\n",
        "        loss = criterion(anc_y, pos_y, neg_y)\n",
        "\n",
        "        # Update the model weights, if the current phase is `train` (if the \n",
        "        #   current phase is `val`, the model weights are not changed)\n",
        "        if phase == 'train':\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        # Add the current batch's loss to `epoch_loss`\n",
        "        epoch_loss += loss.item() * curr_batch_size\n",
        "\n",
        "        # Compute the distance between anchor and positive embeddings of the\n",
        "        #   current batch (`anc_pos_dists`)\n",
        "        anc_pos_dists = (anc_y - pos_y).pow(2).sum(1)\n",
        "        \n",
        "        # Compute the distance between anchor and negative embeddings of the\n",
        "        #   current batch (`anc_neg_dists`)\n",
        "        anc_neg_dists = (anc_y - neg_y).pow(2).sum(1)\n",
        "        \n",
        "        # Compute which anchor-positive embeddings distance were smaller than\n",
        "        #   anchor-negative embeddings distance to `epoch_acc`\n",
        "        epoch_acc += torch.sum(anc_pos_dists + 1 < anc_neg_dists).item()\n",
        "        \n",
        "        # Calculate the current epoch-phase's average loss and correct\n",
        "        #   anchor-positive and anchor-negative embeddings distance rate\n",
        "        #   (`curr_loss` and `curr_acc`)\n",
        "        curr_loss = epoch_loss / n_seen_samples\n",
        "        curr_acc = epoch_acc / n_seen_samples\n",
        "\n",
        "        # Iteratively print on console the number of iterated batches, as well\n",
        "        #   as the current loss and accuracy\n",
        "        dataloader.set_postfix(Epoch='%s/%s' % (epoch+1, n_epochs),\n",
        "            Loss=curr_loss, Acc=curr_acc, refresh=True)\n",
        "      \n",
        "      # Calculate the final loss and accuracy of the current epoch\n",
        "      epoch_loss /= len(datasets[phase])\n",
        "      epoch_acc /= len(datasets[phase])\n",
        "    \n",
        "      # Save new weights if the best loss is achieved in the `val` phase\n",
        "      if phase == 'val' and epoch_loss < best_loss:\n",
        "        best_acc = epoch_acc\n",
        "        best_loss = epoch_loss\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "  \n",
        "  # Apply best weights to the model\n",
        "  model.load_state_dict(best_weights)\n",
        "\n",
        "  return model, best_acc, best_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrQ1R1HpDKEl"
      },
      "source": [
        "## Model testing algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOZog8QA1mOl"
      },
      "source": [
        "def test(model):\n",
        "  '''\n",
        "  Tests the model on an unseen set of samples.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn\n",
        "    the model to be evaluated\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  acc : float\n",
        "    the final accuracy of the model\n",
        "  '''\n",
        "\n",
        "  # Set the model mode to `eval`, as the model weights are not updated\n",
        "  model.eval()\n",
        "\n",
        "  # Keep track of the current accuracy (`acc`)\n",
        "  acc = 0.0\n",
        "  \n",
        "  # `n_seen_samples` computes the number of triplet input samples that were\n",
        "  #   already read\n",
        "  n_seen_samples = 0\n",
        "\n",
        "  # Using tqdm to iteratively keep track on the number of iterated batches on\n",
        "  #   the console\n",
        "  dataloader = tqdm.tqdm(dataloaders['test'], total=len(dataloaders['test']),\n",
        "      position=0, leave=True)\n",
        "  \n",
        "  # The dataloader refering to the test phase is iterated, in order to\n",
        "  #   access all triplets of anchor-positive-negative inputs, denoted by\n",
        "  #   `(anc_x, pos_x, neg_x)`\n",
        "  for anc_x, pos_x, neg_x in dataloader:\n",
        "  \n",
        "    # `curr_batch_size` computes the number of samples in the current batch\n",
        "    #   (this may vary when the current batch is the last one)\n",
        "    curr_batch_size = anc_x.shape[0]\n",
        "  \n",
        "    # Increment the number of seen samples\n",
        "    n_seen_samples += curr_batch_size\n",
        "\n",
        "    # Pass the triplet inputs tensors to the used device (GPU or CPU)\n",
        "    anc_x = anc_x.to(device)\n",
        "    pos_x = pos_x.to(device)\n",
        "    neg_x = neg_x.to(device)\n",
        "\n",
        "    # Use the model to calculate the embeddings of the triplet inputs\n",
        "    #   (`anc_y`, `pos_y` and `neg_y`)\n",
        "    anc_y = model(anc_x)\n",
        "    pos_y = model(pos_x)\n",
        "    neg_y = model(neg_x)\n",
        "\n",
        "    # Compute the distance between anchor and positive embeddings of the\n",
        "    #   current batch (`anc_pos_dists`)\n",
        "    anc_pos_dists = (anc_y - pos_y).pow(2).sum(1)\n",
        "  \n",
        "    # Compute the distance between anchor and negative embeddings of the\n",
        "    #   current batch (`anc_neg_dists`)\n",
        "    anc_neg_dists = (anc_y - neg_y).pow(2).sum(1)\n",
        "  \n",
        "    # Compute which anchor-positive embeddings distance were smaller than\n",
        "    #   anchor-negative embeddings distance to `acc`\n",
        "    acc += torch.sum(anc_pos_dists + 1 < anc_neg_dists).item()\n",
        "\n",
        "    # Calculate the current epoch-phase's correct anchor-positive and\n",
        "    #   anchor-negative embeddings distance rate (`curr_acc`)    \n",
        "    curr_acc = acc / n_seen_samples\n",
        "\n",
        "    # Iteratively print on console the number of iterated batches, as well as\n",
        "    #   the current accuracy\n",
        "    dataloader.set_postfix(Acc=curr_acc, refresh=True)\n",
        "\n",
        "  # Calculate the final accuracy of the test\n",
        "  acc / len(datasets['test'])\n",
        "\n",
        "  return acc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDAzUqbj4RJS"
      },
      "source": [
        "## Sets of preprocessing operations\n",
        "\n",
        "Each set refers to each phase (training, validation and test)\n",
        "\n",
        "- For training phase, samples are randomly cropped to a fixed (224,224) size, randomly flipped horizontally, applied to a torch.Tensor and normalized according to a set of RGB mean and standard deviation values;\n",
        "\n",
        "- For validation phase, samples are centrally cropped to a fixed (224,224) size, applied to a torch.Tensor and normalized according to a set of RGB mean and standard deviation values;\n",
        "\n",
        "- For testing phase, samples are resized to a fixed (256,256) size, centrally cropped to a fixed (224,224) size, applied to a torch.Tensor and normalized according to a set of RGB mean and standard deviation values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgcOVIY4zacj"
      },
      "source": [
        "dataset_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTOWvNd1fp1L"
      },
      "source": [
        "## Setting up training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7aQfMfLzbrU",
        "outputId": "c28a71e3-4f04-4249-99b7-ca546ce3537b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "2dec0f0a5f7b4b31be43a31a80ad5e9f",
            "68540bd20ea74ee4840afa8289434c8c",
            "52454491a6bd4da59bd52f58c5288dc6",
            "578d2847af19497b9c8548b0bdaec955",
            "4643c9ed38cd4e91b317bb886b541c2a",
            "c95ecdbed0d3466190846865a90842b8",
            "1a418aa492fb4194aeadf0ab7bc8dfa2",
            "df7af08bcd4c4952ab314bfdac1780d5"
          ]
        }
      },
      "source": [
        "# Path containing the dog breed training dataset\n",
        "dataset_path = os.path.join(root, 'dogs', 'train')\n",
        "\n",
        "# Split ratio of the training, validation and test portions (these portions sum\n",
        "#   up to 1.0)\n",
        "split_ratio = [0.7, 0.15, 0.15]\n",
        "\n",
        "# Get dataset labels, splitted for each phase\n",
        "dataset_labels = get_dataset_split_labels(dataset_path, split_ratio)\n",
        "\n",
        "# Creating PyTorch dataset instance for each phase (training, validation and\n",
        "#   test)\n",
        "datasets = {\n",
        "    'train': ImageDataset(dataset_path, dataset_labels['train'],\n",
        "        dataset_transforms['train']),\n",
        "    'val': ImageDataset(dataset_path, dataset_labels['val'],\n",
        "        dataset_transforms['val']),\n",
        "    'test': ImageDataset(dataset_path, dataset_labels['test'],\n",
        "        dataset_transforms['test']) }\n",
        "\n",
        "# Number of embeddings to be outputted by the model\n",
        "n_embeddings = 128\n",
        "\n",
        "# Number of epochs for the model to be trained\n",
        "n_epochs = 20\n",
        "\n",
        "# Batch size for each phase\n",
        "batch_size = 16\n",
        "\n",
        "# Number of workers for multiprocessing the data loading\n",
        "n_workers = 8\n",
        "\n",
        "# Instantiate CNN embeddings extractor model\n",
        "model = embedder_model(n_embeddings)\n",
        "model = torch.jit.script(model).to(device)\n",
        "\n",
        "# # Use Triplet Margin loss\n",
        "criterion = torch.nn.TripletMarginLoss()\n",
        "# Use Lossless Triplet loss\n",
        "# criterion = LosslessTripletLoss(n_embeddings)\n",
        "\n",
        "# Use SGD as the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Create data loaders for the training, validation and testing steps\n",
        "#   (Also shuffling samples for unbiased performance)\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(datasets['train'],\n",
        "        batch_size=batch_size, num_workers=n_workers, shuffle=True),\n",
        "    'val': torch.utils.data.DataLoader(datasets['val'], batch_size=batch_size,\n",
        "        num_workers=n_workers, shuffle=True),\n",
        "    'test': torch.utils.data.DataLoader(datasets['test'], batch_size=batch_size,\n",
        "        num_workers=n_workers, shuffle=True) }\n",
        "\n",
        "# Path where to save the trained model\n",
        "trained_model_ckpt_path = os.path.join(root, 'models', 'embedder.pth')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dec0f0a5f7b4b31be43a31a80ad5e9f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eKQZSuEfr_H"
      },
      "source": [
        "## Executing training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNqWGewA_LT",
        "outputId": "536f3fcb-e9f6-4f84-9638-a63381824074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "# Generate a trained model (`trained_model`), as well as its accuracy\n",
        "#   (`val_acc`) and loss (`val_loss`)\n",
        "trained_model, val_acc, val_loss = train(model, criterion, optimizer, n_epochs)\n",
        "\n",
        "# Save model to `trained_model_ckpt_path`\n",
        "torch.save({\n",
        "    'state_dict': trained_model.state_dict(),\n",
        "    'acc': val_acc,\n",
        "    'loss': val_loss,\n",
        "    'n_embeddings': n_embeddings }, trained_model_ckpt_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [21:15<00:00,  1.68s/it, Acc=0.701, Epoch=1/20, Loss=0.364]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:26<00:00,  1.85it/s, Acc=0.923, Epoch=1/20, Loss=0.14]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.869, Epoch=2/20, Loss=0.233]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.65it/s, Acc=0.924, Epoch=2/20, Loss=0.138]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:44<00:00,  1.09s/it, Acc=0.888, Epoch=3/20, Loss=0.209]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.934, Epoch=3/20, Loss=0.126]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:44<00:00,  1.09s/it, Acc=0.891, Epoch=4/20, Loss=0.202]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.65it/s, Acc=0.949, Epoch=4/20, Loss=0.106]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:44<00:00,  1.09s/it, Acc=0.898, Epoch=5/20, Loss=0.193]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.946, Epoch=5/20, Loss=0.109]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.901, Epoch=6/20, Loss=0.189]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.957, Epoch=6/20, Loss=0.0913]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.91, Epoch=7/20, Loss=0.175]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.63it/s, Acc=0.955, Epoch=7/20, Loss=0.0901]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.907, Epoch=8/20, Loss=0.176]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.958, Epoch=8/20, Loss=0.0859]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:46<00:00,  1.09s/it, Acc=0.914, Epoch=9/20, Loss=0.17]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:01<00:00,  2.61it/s, Acc=0.949, Epoch=9/20, Loss=0.0989]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:44<00:00,  1.09s/it, Acc=0.912, Epoch=10/20, Loss=0.169]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.65it/s, Acc=0.956, Epoch=10/20, Loss=0.0897]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.917, Epoch=11/20, Loss=0.164]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:59<00:00,  2.67it/s, Acc=0.958, Epoch=11/20, Loss=0.0915]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:44<00:00,  1.09s/it, Acc=0.918, Epoch=12/20, Loss=0.16]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.957, Epoch=12/20, Loss=0.0828]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.916, Epoch=13/20, Loss=0.166]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.963, Epoch=13/20, Loss=0.082]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.921, Epoch=14/20, Loss=0.16]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.65it/s, Acc=0.962, Epoch=14/20, Loss=0.0835]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.922, Epoch=15/20, Loss=0.158]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.65it/s, Acc=0.962, Epoch=15/20, Loss=0.0823]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:45<00:00,  1.09s/it, Acc=0.924, Epoch=16/20, Loss=0.156]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.63it/s, Acc=0.957, Epoch=16/20, Loss=0.0803]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:57<00:00,  1.11s/it, Acc=0.924, Epoch=17/20, Loss=0.153]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.63it/s, Acc=0.961, Epoch=17/20, Loss=0.084]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:57<00:00,  1.11s/it, Acc=0.926, Epoch=18/20, Loss=0.153]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.64it/s, Acc=0.961, Epoch=18/20, Loss=0.0775]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:57<00:00,  1.11s/it, Acc=0.924, Epoch=19/20, Loss=0.157]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:00<00:00,  2.63it/s, Acc=0.964, Epoch=19/20, Loss=0.0805]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 757/757 [13:57<00:00,  1.11s/it, Acc=0.928, Epoch=20/20, Loss=0.148]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:01<00:00,  2.62it/s, Acc=0.965, Epoch=20/20, Loss=0.0745]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAhlPr80fvj8"
      },
      "source": [
        "## Executing test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6birUOlDAZEO",
        "outputId": "2bd07ad8-42b9-4df2-d087-87873d8dfebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Instantiating the architecture of the model\n",
        "trained_model = embedder_model(n_embeddings)\n",
        "\n",
        "# Load weights from the trained model\n",
        "trained_model.load_state_dict(torch.load(trained_model_ckpt_path)['state_dict'])\n",
        "trained_model.eval()\n",
        "trained_model = torch.jit.script(trained_model).to(device)\n",
        "\n",
        "# Perform testing and getting test accuracy (`test_acc`)\n",
        "test_acc = test(trained_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [06:30<00:00,  2.44s/it, Acc=0.976]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}